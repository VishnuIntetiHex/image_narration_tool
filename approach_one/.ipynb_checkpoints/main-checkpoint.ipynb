{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee9a437c-3b16-44fe-9a15-acc486a2b889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "import scipy\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104a5cac-e634-445b-a1d5-f13f8a207ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f58a268-da14-4a2a-8f8e-e6dbb524b29e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m BASE_DIR = \u001b[43mPath\u001b[49m(os.getcwd())\n\u001b[32m      3\u001b[39m PROJECT_ROOT = BASE_DIR.parent\n\u001b[32m      5\u001b[39m cats_img = Image.open(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPROJECT_ROOT\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/images/image_2.jpeg\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "BASE_DIR = Path(os.getcwd())\n",
    "\n",
    "PROJECT_ROOT = BASE_DIR.parent\n",
    "\n",
    "cats_img = Image.open(f\"{PROJECT_ROOT}/images/image_2.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9902d20-39c5-4467-990c-19e1882a56af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b558815-0516-499c-a5ff-7202f997d3e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nTimmBackbone requires the timm library but it was not found in your environment. You can install it with pip:\n`pip install timm`. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m image_segmentation = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mobject-detection\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfacebook/detr-resnet-50\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      4\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m detections = image_segmentation(cats_img)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(detections)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\ai_program\\submissions\\image_narration_tool\\.venv\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:836\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m    834\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    835\u001b[39m     model_classes = targeted_task[\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m836\u001b[39m     model = \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m        \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    845\u001b[39m hub_kwargs[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m] = model.config._commit_hash\n\u001b[32m    847\u001b[39m \u001b[38;5;66;03m# Check which preprocessing classes the pipeline uses\u001b[39;00m\n\u001b[32m    848\u001b[39m \u001b[38;5;66;03m# None values indicate optional classes that the pipeline can run without, we don't raise errors if loading fails\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\ai_program\\submissions\\image_narration_tool\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:232\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(model, config, model_classes, task, **model_kwargs)\u001b[39m\n\u001b[32m    229\u001b[39m kwargs = model_kwargs.copy()\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m     model = \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m     \u001b[38;5;66;03m# Stop loading on the first successful load.\u001b[39;00m\n\u001b[32m    234\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\ai_program\\submissions\\image_narration_tool\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:374\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    372\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    373\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m374\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    378\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    379\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    380\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\ai_program\\submissions\\image_narration_tool\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:4030\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4027\u001b[39m config = copy.deepcopy(config)  \u001b[38;5;66;03m# We do not want to modify the config inplace in from_pretrained.\u001b[39;00m\n\u001b[32m   4028\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(model_init_context):\n\u001b[32m   4029\u001b[39m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4030\u001b[39m     model = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4032\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# replace module with quantized modules (does not touch weights)\u001b[39;00m\n\u001b[32m   4033\u001b[39m         hf_quantizer.preprocess_model(\n\u001b[32m   4034\u001b[39m             model=model,\n\u001b[32m   4035\u001b[39m             dtype=dtype,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4038\u001b[39m             use_kernels=use_kernels,\n\u001b[32m   4039\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\ai_program\\submissions\\image_narration_tool\\.venv\\Lib\\site-packages\\transformers\\models\\detr\\modeling_detr.py:1305\u001b[39m, in \u001b[36mDetrForObjectDetection.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m   1302\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(config)\n\u001b[32m   1304\u001b[39m \u001b[38;5;66;03m# DETR encoder-decoder model\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1305\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[43mDetrModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1307\u001b[39m \u001b[38;5;66;03m# Object detection heads\u001b[39;00m\n\u001b[32m   1308\u001b[39m \u001b[38;5;28mself\u001b[39m.class_labels_classifier = nn.Linear(\n\u001b[32m   1309\u001b[39m     config.d_model, config.num_labels + \u001b[32m1\u001b[39m\n\u001b[32m   1310\u001b[39m )  \u001b[38;5;66;03m# We add one for the \"no object\" class\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\ai_program\\submissions\\image_narration_tool\\.venv\\Lib\\site-packages\\transformers\\models\\detr\\modeling_detr.py:1115\u001b[39m, in \u001b[36mDetrModel.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m   1112\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: DetrConfig):\n\u001b[32m   1113\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(config)\n\u001b[32m-> \u001b[39m\u001b[32m1115\u001b[39m     \u001b[38;5;28mself\u001b[39m.backbone = \u001b[43mDetrConvEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1117\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config.position_embedding_type == \u001b[33m\"\u001b[39m\u001b[33msine\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1118\u001b[39m         \u001b[38;5;28mself\u001b[39m.position_embedding = DetrSinePositionEmbedding(config.d_model // \u001b[32m2\u001b[39m, normalize=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\ai_program\\submissions\\image_narration_tool\\.venv\\Lib\\site-packages\\transformers\\models\\detr\\modeling_detr.py:261\u001b[39m, in \u001b[36mDetrConvEncoder.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m    259\u001b[39m \u001b[38;5;28mself\u001b[39m.config = config\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m backbone = \u001b[43mload_backbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[38;5;28mself\u001b[39m.intermediate_channel_sizes = backbone.channels\n\u001b[32m    264\u001b[39m \u001b[38;5;66;03m# replace batch norm by frozen batch norm\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\ai_program\\submissions\\image_narration_tool\\.venv\\Lib\\site-packages\\transformers\\backbone_utils.py:340\u001b[39m, in \u001b[36mload_backbone\u001b[39m\u001b[34m(config)\u001b[39m\n\u001b[32m    338\u001b[39m     backbone = AutoBackbone.from_config(config=config)\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m     backbone = \u001b[43mAutoBackbone\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackbone_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m backbone\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\ai_program\\submissions\\image_narration_tool\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:238\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_config\u001b[39m\u001b[34m(cls, config, **kwargs)\u001b[39m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._model_mapping:\n\u001b[32m    237\u001b[39m     model_class = _get_model_class(config, \u001b[38;5;28mcls\u001b[39m._model_mapping)\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    241\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    242\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    243\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\ai_program\\submissions\\image_narration_tool\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:1474\u001b[39m, in \u001b[36mPreTrainedModel._from_config\u001b[39m\u001b[34m(cls, config, **kwargs)\u001b[39m\n\u001b[32m   1472\u001b[39m \u001b[38;5;66;03m# Instantiate the model\u001b[39;00m\n\u001b[32m   1473\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[32m-> \u001b[39m\u001b[32m1474\u001b[39m     model = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1476\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\ai_program\\submissions\\image_narration_tool\\.venv\\Lib\\site-packages\\transformers\\models\\timm_backbone\\modeling_timm_backbone.py:43\u001b[39m, in \u001b[36mTimmBackbone.__init__\u001b[39m\u001b[34m(self, config, **kwargs)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtimm\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config.backbone \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     46\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mbackbone is not set in the config. Please set it to a timm model name.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\ai_program\\submissions\\image_narration_tool\\.venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1878\u001b[39m, in \u001b[36mrequires_backends\u001b[39m\u001b[34m(obj, backends)\u001b[39m\n\u001b[32m   1875\u001b[39m         failed.append(msg.format(name))\n\u001b[32m   1877\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[32m-> \u001b[39m\u001b[32m1878\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(failed))\n",
      "\u001b[31mImportError\u001b[39m: \nTimmBackbone requires the timm library but it was not found in your environment. You can install it with pip:\n`pip install timm`. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "image_segmentation = pipeline(\n",
    "    task=\"object-detection\",\n",
    "    model=\"facebook/detr-resnet-50\"\n",
    ")\n",
    "\n",
    "detections = image_segmentation(cats_img)\n",
    "\n",
    "print(detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e92261-e8b9-4c14-99f7-21f959e00d23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5e27b7-4fa1-4499-b70e-067a342b8862",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [x['label'] for x in detections]\n",
    "\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3144a0fb-c403-4190-a3d5-ef0f2500aa1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3813fb86-fd9b-4205-854e-953102b49ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = {}\n",
    "\n",
    "for label in labels:\n",
    "    if objects.get(label):\n",
    "        objects[label] += 1\n",
    "    else:\n",
    "        objects[label] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c785634-116b-4bda-a963-b4e9b008d9a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d846e420-9a33-4ee4-bff0-b4a41f07ae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I see \"\n",
    "\n",
    "for object in objects.keys():\n",
    "    sentence += f\"{objects[object]} {object}\" + 's ' if objects[object] >1 else ''\n",
    "\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3163941-ce0f-4d70-9d54-27104ded2679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624d56a7-189b-4ed6-a531-208ba26c3ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_audio = pipeline(\n",
    "    task=\"text-to-audio\",\n",
    "    model=\"suno/bark-small\"\n",
    ")\n",
    "\n",
    "gen_audio = speech_audio(sentence)\n",
    "\n",
    "scipy.io.wavfile.write(\"traffic_sentence.wav\", rate=gen_audio[\"sampling_rate\"], data=gen_audio[\"audio\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
